{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Group Project 2: Data Structures in Action Table of Contents & Text Analysis**\n",
        "\n",
        "Data Structures and Algorithms, Fall 2025"
      ],
      "metadata": {
        "id": "PZS8wP_sagaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **README**\n",
        "\n",
        "**Group Number:**  \n",
        "\n",
        "8\n",
        "\n",
        "**Group Members:**  \n",
        "\n",
        "Samanwita Mukherjee, Andrea Caceres, Chushmitha Battula, Sam Goodell\n",
        "\n",
        "**Github Repo:**\n",
        "\n",
        "https://github.com/lavender2412/GP2_DSA_Data_Structures_in_Action_Table_of_Contents_-_Text_Analysis.git\n",
        "\n",
        "**Objectives:**\n",
        "\n",
        "(1) Implement tree data structures and relevant methods to store, traverse, and organize a table of contents from a technical textbook.  \n",
        "(2) Preprocess and analyze a public domain novel to investigate letter/word frequencies, bigrams/trigrams, and a variety of sentence structure metrics.\n",
        "\n",
        "**Datasets:**\n",
        "\n",
        "Question 1: Data Science and Predictive Analytics (2018) by Ivo D. Dinov (Table of Contents)\n",
        "\n",
        "Question 2: Wuthering Heights (1847) by Emily Bronte\n",
        "\n",
        "**Team Roles:**\n",
        "\n",
        "Question 1 leads: Sam Goodell and Chushmitha Battula\n",
        "\n",
        "Question 2 leads: Andrea Caceres and Samanwita Mukherjee\n"
      ],
      "metadata": {
        "id": "99zrDKgFgh8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: Build a Table of Contents (Tree)**\n",
        "\n",
        "**Task:** Create a hierarchical Table of Contents (TOC) from a Data Science / AI technical book\n",
        "\n",
        "**Textbook Title:** Data Science and Predictive Analytics (2020)\n",
        "\n",
        "**Author:** Ivo D. Dinov"
      ],
      "metadata": {
        "id": "apSM4lkVSht9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import and Clean Table of Contents:**"
      ],
      "metadata": {
        "id": "uSCl43jNTuaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import string\n",
        "import re as re"
      ],
      "metadata": {
        "id": "tjGz32x6UYlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gdown\n",
        "\n",
        "import gdown, os\n",
        "\n",
        "file_id = \"17TUKK5KNXvE3tr3STZHNK_QL4Wpwcu_Z\"\n",
        "file_name = \"table_of_contents_corrected.txt\"\n",
        "\n",
        "if not os.path.exists(file_name):\n",
        "    gdown.download(id=file_id, output=file_name, quiet=False)\n",
        "\n",
        "with open(file_name, encoding=\"utf-8\") as file:\n",
        "    contents = file.read()\n",
        "\n",
        "cleaned_contents = []\n",
        "\n",
        "#split text at each new line\n",
        "for line in contents.splitlines():\n",
        "    #remove leading and trailing spaces\n",
        "    strip = line.strip()\n",
        "    #remove all instances of 2 or more periods\n",
        "    replace_period = re.sub(r'\\.{2,}',\"\", strip)\n",
        "    #create list by splitting at instances of 2 spaces\n",
        "    split = replace_period.split(\"  \")\n",
        "    #keep lists with lengths greater than 2, append\n",
        "    if len(split) > 2:\n",
        "        cleaned_contents.append(split)"
      ],
      "metadata": {
        "id": "N9a7o-e9T39K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd10b93-2076-4538-eac7-e08dcdc3e486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17TUKK5KNXvE3tr3STZHNK_QL4Wpwcu_Z\n",
            "To: /content/table_of_contents_corrected.txt\n",
            "100%|██████████| 56.0k/56.0k [00:00<00:00, 24.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Create Node Class and Methods:**"
      ],
      "metadata": {
        "id": "6U1BaCFnYITz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create node class\n",
        "class Node:\n",
        "    def __init__(self, book, section, title, page):\n",
        "        self.section = section\n",
        "        self.title = title\n",
        "        self.page = page\n",
        "        self.book = book\n",
        "        #create section order, count of periods in section + 1\n",
        "        self.section_order = section.count(\".\") + 1 if section else 0\n",
        "        self.total = 0\n",
        "        self.children = []\n",
        "\n",
        "    #assumes sorted table of contents\n",
        "    def insert_section(self, section, title, page):\n",
        "        new_section = Node(self.book, section, title, page)\n",
        "        new_order = new_section.section_order\n",
        "        #if section order is 1 greater than root/current node, appending to current node's children\n",
        "        if new_order == self.section_order + 1:\n",
        "            self.children.append(new_section)\n",
        "            #increment total nodes\n",
        "            self.total += 1\n",
        "            return new_section\n",
        "        #if not a direct child, loop through each child node\n",
        "        for children in self.children:\n",
        "            #check if child is in section lineage\n",
        "            if section.startswith(children.section + \".\"):\n",
        "                #if so, recurse with child node\n",
        "                inserted = children.insert_section(section, title, page)\n",
        "                #increment total nodes\n",
        "                if inserted:\n",
        "                    self.total+=1\n",
        "                return inserted\n",
        "        print(f\"Could not add {new_section.title}, {new_section.section}\")\n",
        "\n",
        "    #loop through cleaned list of contents\n",
        "    def insert_many(self, arr):\n",
        "        for val in arr:\n",
        "            self.insert_section(val[0], val[1], val[2])\n",
        "\n",
        "\n",
        "    def print_toc(self, mode):\n",
        "      # will store the values in list instead of printing them all\n",
        "        lines = []\n",
        "        def traverse(node, chapter_hierarchy, level):\n",
        "          # wit\n",
        "            if node.title:\n",
        "              # if the node has an existing not none title, we concatenate the hierarchy to use it later in the numbered indentation\n",
        "                number_str = \".\".join(str(num) for num in chapter_hierarchy)\n",
        "              # level helps with keeps track of how many indent positions should we apply based on the level in TOC\n",
        "                indent = \"  \" * level\n",
        "                if mode == \"plain\":\n",
        "              # we just print the book TOC titles, when asked for plain formatting\n",
        "                    lines.append(node.title)\n",
        "                elif mode == \"indented\":\n",
        "              # we use the pre-calculated indent variable and concatenate it to the title, when its asked to print with indentation format\n",
        "                    lines.append(indent + node.title)\n",
        "              # to print with number and indentation we use the above 2 pre-calculated variables to display the contents as requried\n",
        "                elif mode == \"numbered\":\n",
        "                    lines.append(indent + (f\"{number_str} \" if number_str else \"\") + node.title)\n",
        "\n",
        "           # we loop over the children [], and perform preorder traversal here, to print the current node first and look into its children node, to maintain the TOC structure\n",
        "\n",
        "            for i, child in enumerate(node.children, 1):\n",
        "\n",
        "                traverse(child, chapter_hierarchy + [i], level + 1)\n",
        "\n",
        "        # initial function call, that will execute the function\n",
        "\n",
        "        traverse(self, [], 0)\n",
        "\n",
        "        # returns the list of lines we required from the TOC, which will be used in the show_toc() method to display it correctly\n",
        "\n",
        "        return lines\n",
        "\n",
        "    def show_toc(book, mode):\n",
        "      # method used to print the TOC in the colab cells without any interrupted\n",
        "        toc_lines = book.print_toc(mode)\n",
        "      # adds the line break after each line of the content to display it in html content\n",
        "        html_block = \"<br>\".join(toc_lines)\n",
        "        display(HTML(f\"\"\"<div style=\"max-height: 400px; overflow-y: auto; white-space: pre; font-family: monospace;\">{html_block}</div>\"\"\"))\n",
        "\n",
        "\n",
        "    def tree_height(self):\n",
        "        #initialize empty array\n",
        "        heights_arr = []\n",
        "        #check for children of root/current node\n",
        "        if self.children:\n",
        "            #iterate over each child node\n",
        "            for children in self.children:\n",
        "                #recurse to each leaf\n",
        "                height = children.tree_height()\n",
        "                #after recursion, append height of each leaf\n",
        "                heights_arr.append(height)\n",
        "        #base case - return 0 because root node has height 0\n",
        "        else:\n",
        "            return 0\n",
        "        # return max height + 1 to increment each recurion\n",
        "        return 1 + max(heights_arr)\n",
        "\n",
        "\n",
        "    def tree_depth(self, title):\n",
        "        # using dfs approach here as we want to have the minimal traversal made to identify the actual title\n",
        "        # we start with 0 at the root and the depth is 0 at root\n",
        "        if self.book == title:\n",
        "          return 0\n",
        "        def dfs(node, depth):\n",
        "          # check with title and return the depth when founded\n",
        "            if node.title == title:\n",
        "                return depth\n",
        "            for child in node.children:\n",
        "          # we loop through all the children [] and increase the depth by 1, with each recursive call\n",
        "                result = dfs(child, depth + 1)\n",
        "                if result is not None:\n",
        "                    return result\n",
        "            return None\n",
        "        # this will call our dfs method\n",
        "        return dfs(self, 0)\n"
      ],
      "metadata": {
        "id": "qIPKCImgYFdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Methods:**"
      ],
      "metadata": {
        "id": "xNan60AZYkKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create root (book)\n",
        "book = Node(book = \"Data Science and Predictive Analytics\", section = None, title = None, page = None)\n",
        "\n",
        "#Insert all elements\n",
        "book.insert_many(cleaned_contents)\n",
        "\n",
        "#Show total elements in TOC\n",
        "print(f\"Total Sections in Table of Contents: {book.total}\")\n",
        "\n",
        "#Find tree height\n",
        "print(f\"Max Height in Table of Contents: {book.tree_height()}\")\n",
        "\n",
        "#Find depth with the title\n",
        "print(f\"Depth of Title: {book.tree_depth('Parkinson’s Disease')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2LR6uQhYjO0",
        "outputId": "73c3d2d8-7532-494a-9e3e-8c4aaa1b3a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sections in Table of Contents: 579\n",
            "Max Height in Table of Contents: 3\n",
            "Depth of Title: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can validate the above outputs, through the below show_toc() method.\n"
      ],
      "metadata": {
        "id": "EwUS7EOQ59Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book.show_toc(\"plain\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "reV08PjSl7os",
        "outputId": "acd996ef-00da-44cb-e9a2-08cea66d1df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"max-height: 400px; overflow-y: auto; white-space: pre; font-family: monospace;\">Motivation<br>DSPA Mission and Objectives<br>Examples of Driving Motivational Problems and Challenges<br>Alzheimer’s Disease<br>Parkinson’s Disease<br>Drug and Substance Use<br>Amyotrophic Lateral Sclerosis<br>Normal Brain Visualization<br>Neurodegeneration<br>Genetic Forensics: 2013–2016 Ebola Outbreak<br>Next Generation Sequence (NGS) Analysis<br>Neuroimaging-Genetics<br>Common Characteristics of Big (Biomedical and Health) Data<br>Data Science<br>Predictive Analytics<br>High-Throughput Big Data Analytics<br>Examples of Data Repositories, Archives, and Services<br>DSPA Expectations<br>Foundations of R<br>Why Use R?<br>Getting Started<br>Install Basic Shell-Based R<br>GUI Based R Invocation (RStudio)<br>RStudio GUI Layout<br>Some Notes<br>Help<br>Simple Wide-to-Long Data format Translation<br>Data Generation<br>Input/Output (I/O)<br>Slicing and Extracting Data<br>Variable Conversion<br>Variable Information<br>Data Selection and Manipulation<br>Math Functions<br>Matrix Operations<br>Advanced Data Processing<br>Strings<br>Plotting<br>QQ Normal Probability Plot<br>Low-Level Plotting Commands<br>Graphics Parameters<br>Optimization and Model Fitting<br>Statistics<br>Distributions<br>Programming<br>Data Simulation Primer<br>Appendix<br>HTML SOCR Data Import<br>R Debugging<br>Assignments: 2. R Foundations<br>Confirm that You Have Installed R/RStudio<br>Long-to-Wide Data Format Translation<br>Data Frames<br>Data Stratification<br>Simulation<br>Programming<br>Managing Data in R<br>Saving and Loading R Data Structures<br>Importing and Saving Data from CSV Files<br>Exploring the Structure of Data<br>Exploring Numeric Variables<br>Measuring the Central Tendency: Mean, Median, Mode<br>Measuring Spread: Quartiles and the Five-Number Summary<br>Visualizing Numeric Variables: Boxplots<br>Visualizing Numeric Variables: Histograms<br>Understanding Numeric Data: Uniform and Normal Distributions<br>Measuring Spread: Variance and Standard Deviation<br>Exploring Categorical Variables<br>Exploring Relationships Between Variables<br>Missing Data<br>Simulate Some Real Multivariate Data<br>TBI Data Example<br>Imputation via Expectation-Maximization<br>Parsing Webpages and Visualizing Tabular HTML Data<br>Cohort-Rebalancing (for Imbalanced Groups)<br>Appendix<br>Importing Data from SQL Databases<br>R Code Fragments<br>Assignments: 3. Managing Data in R<br>Import, Plot, Summarize and Save Data<br>Explore some Bivariate Relations in the Data<br>Missing Data<br>Surface Plots<br>Unbalanced Designs<br>Aggregate Analysis<br>Data Visualization<br>Common Questions<br>Classification of Visualization Methods<br>Composition<br>Histograms and Density Plots<br>Pie Chart<br>Heat Map<br>Comparison<br>Paired Scatter Plots<br>Jitter Plot<br>Bar Plots<br>Trees and Graphs<br>Correlation Plots<br>Relationships<br>Line Plots Using ggplot<br>Density Plots<br>Distributions<br>2D Kernel Density and 3D Surface Plots<br>Multiple 2D Image Surface Plots<br>3D and 4D Visualizations<br>Appendix<br>Hands-on Activity (Health Behavior Risks)<br>Additional ggplot Examples<br>Assignments 4: Data Visualization<br>Common Plots<br>Trees and Graphs<br>Exploratory Data Analytics (EDA)<br>Linear Algebra & Matrix Computing<br>Matrices (Second Order Tensors)<br>Create Matrices<br>Adding Columns and Rows<br>Matrix Subscripts<br>Matrix Operations<br>Addition<br>Subtraction<br>Multiplication<br>Element-wise Division<br>Transpose<br>Multiplicative Inverse<br>Matrix Algebra Notation<br>Linear Models<br>Solving Systems of Equations<br>The Identity Matrix<br>Scalars, Vectors and Matrices<br>Sample Statistics (Mean, Variance)<br>Least Square Estimation<br>Eigenvalues and Eigenvectors<br>Other Important Functions<br>Matrix Notation (Another View)<br>Multivariate Linear Regression<br>Sample Covariance Matrix<br>Assignments: 5. Linear Algebra & Matrix Computing<br>How Is Matrix Multiplication Defined?<br>Scalar Versus Matrix Multiplication<br>Matrix Equations<br>Least Square Estimation<br>Matrix Manipulation<br>Matrix Transpose<br>Sample Statistics<br>Least Square Estimation<br>Eigenvalues and Eigenvectors<br>Dimensionality Reduction<br>Example: Reducing 2D to 1D<br>Matrix Rotations<br>Notation<br>Summary (PCA vs. ICA vs. FA)<br>Principal Component Analysis (PCA)<br>Principal Components<br>Independent Component Analysis (ICA)<br>Factor Analysis (FA)<br>Singular Value Decomposition (SVD)<br>SVD Summary<br>Case Study for Dimension Reduction (Parkinson’s Disease)<br>Assignments: 6. Dimensionality Reduction<br>Parkinson’s Disease Example<br>Allometric Relations in Plants Example<br>Lazy Learning: Classification Using Nearest Neighbors<br>Motivation<br>The kNN Algorithm Overview<br>Distance Function and Dummy Coding<br>Ways to Determine k<br>Rescaling of the Features<br>Rescaling Formulas<br>Case Study<br>Step 1: Collecting Data<br>Step 2: Exploring and Preparing the Data<br>Normalizing Data<br>Data Preparation: Creating Training and Testing Datasets<br>Step 3: Training a Model On the Data<br>Step 4: Evaluating Model Performance<br>Step 5: Improving Model Performance<br>Testing Alternative Values of k<br>Quantitative Assessment (Tables 7.2 and 7.3)<br>Assignments: 7. Lazy Learning: Classification Using Nearest Neighbors<br>Traumatic Brain Injury (TBI)<br>Parkinson’s Disease<br>KNN Classification in a High Dimensional Space<br>KNN Classification in a Low Dimensional Space<br>Probabilistic Learning: Classification Using Naive Bayes<br>Overview of the Naive Bayes Algorithm<br>Assumptions<br>Bayes Formula<br>The Laplace Estimator<br>Case Study: Head and Neck Cancer Medication<br>Step 1: Collecting Data<br>Step 2: Exploring and Preparing the Data<br>Step 3: Training a Model on the Data<br>Step 4: Evaluating Model Performance<br>Step 5: Improving Model Performance<br>Step 6: Compare Naive Bayesian against LDA<br>Practice Problem<br>Assignments 8: Probabilistic Learning: Classification Using Naive Bayes<br>Explain These Two Concepts<br>Analyzing Textual Data<br>Decision Tree Divide and Conquer Classification<br>Motivation<br>Hands-on Example: Iris Data<br>Decision Tree Overview<br>Divide and Conquer<br>Entropy<br>Misclassification Error and Gini Index<br>C5.0 Decision Tree Algorithm<br>Pruning the Decision Tree<br>Case Study 1: Quality of Life and Chronic Disease<br>Step 1: Collecting Data<br>Step 2: Exploring and Preparing the Data<br>Step 3: Training a Model On the Data<br>Step 4: Evaluating Model Performance<br>Step 5: Trial Option<br>Loading the Misclassification Error Matrix<br>Parameter Tuning<br>Compare Different Impurity Indices<br>Classification Rules<br>Separate and Conquer<br>The One Rule Algorithm<br>The RIPPER Algorithm<br>Case Study 2: QoL in Chronic Disease (Take 2)<br>Step 3: Training a Model on the Data<br>Step 4: Evaluating Model Performance<br>Step 5: Alternative Model1<br>Step 5: Alternative Model2<br>Practice Problem<br>Assignments 9: Decision Tree Divide and Conquer Classification<br>Explain These Concepts<br>Decision Tree Partitioning<br>Forecasting Numeric Data Using Regression Models<br>Understanding Regression<br>Simple Linear Regression<br>Ordinary Least Squares Estimation<br>Model Assumptions<br>Correlations<br>Multiple Linear Regression<br>Case Study 1: Baseball Players<br>Step 1: Collecting Data<br>Step 2: Exploring and Preparing the Data<br>Exploring Relationships Among Features: The Correlation Matrix<br>Visualizing Relationships Among Features: The Scatterplot Matrix<br>Step 3: Training a Model on the Data<br>Step 4: Evaluating Model Performance<br>Step 5: Improving Model Performance<br>Model Specification: Adding Non-linear Relationships<br>Transformation: Converting a Numeric Variable to a Binary Indicator<br>Model Specification: Adding Interaction Effects<br>Understanding Regression Trees and Model Trees<br>Adding Regression to Trees<br>Case Study 2: Baseball Players (Take 2)<br>Step 2: Exploring and Preparing the Data<br>Step 3: Training a Model On the Data<br>Visualizing Decision Trees<br>Step 4: Evaluating Model Performance<br>Measuring Performance with Mean Absolute Error<br>Step 5: Improving Model Performance<br>Practice Problem: Heart Attack Data<br>Assignments 10: Forecasting Numeric Data Using Regression Models<br>Black Box Machine-Learning Methods: Neural Networks and Support Vector Machines<br>Understanding Neural Networks<br>From Biological to Artificial Neurons<br>Activation Functions<br>Network Topology<br>The Direction of Information Travel<br>The Number of Nodes in Each Layer<br>Training Neural Networks with Backpropagation<br>Case Study 1: Google Trends and the Stock Market: Regression<br>Step 1: Collecting Data<br>Step 2: Exploring and Preparing the Data<br>Step 3: Training a Model on the Data<br>Step 4: Evaluating Model Performance<br>Step 5: Improving Model Performance<br>Step 6: Adding Additional Layers<br>Simple NN Demo: Learning to Compute √<br>Case Study 2: Google Trends and the Stock Market – Classification<br>Support Vector Machines (SVM)<br>Classification with Hyperplanes<br>Case Study 3: Optical Character Recognition (OCR)<br>Step 1: Prepare and Explore the Data<br>Step 2: Training an SVM Model<br>Step 3: Evaluating Model Performance<br>Step 4: Improving Model Performance<br>Case Study 4: Iris Flowers<br>Step 1: Collecting Data<br>Step 2: Exploring and Preparing the Data<br>Step 3: Training a Model on the Data<br>Step 4: Evaluating Model Performance<br>Step 5: RBF Kernel Function<br>Parameter Tuning<br>Improving the Performance of Gaussian Kernels<br>Practice<br>Problem 1: Google Trends and the Stock Market<br>Problem 2: Quality of Life and Chronic Disease<br>Appendix<br>Assignments 11: Black Box Machine-Learning Methods: Neural Networks and Support Vector Machines<br>Learn and Predict a Power-Function<br>Pediatric Schizophrenia Study<br>Apriori Association Rules Learning<br>Association Rules<br>The Apriori Algorithm for Association Rule Learning<br>Measuring Rule Importance by Using Support and Confidence<br>Building a Set of Rules with the Apriori Principle<br>A Toy Example<br>Case Study 1: Head and Neck Cancer Medications<br>Step 1: Collecting Data<br>Step 2: Exploring and Preparing the Data<br>Step 3: Training a Model on the Data<br>Step 4: Evaluating Model Performance<br>Step 5: Improving Model Performance<br>Practice Problems: Groceries<br>Summary<br>Assignments 12: Apriori Association Rules Learning<br>k-Means Clustering<br>Clustering as a Machine Learning Task<br>Silhouette Plots<br>The k-Means Clustering Algorithm<br>Using Distance to Assign and Update Clusters<br>Choosing the Appropriate Number of Clusters<br>Case Study 1: Divorce and Consequences on Young Adults<br>Step 1: Collecting Data<br>Step 2: Exploring and Preparing the Data<br>Step 3: Training a Model on the Data<br>Step 4: Evaluating Model Performance<br>Step 5: Usage of Cluster Information<br>Model Improvement<br>Tuning the Parameter k<br>Case Study 2: Pediatric Trauma<br>Step 1: Collecting Data<br>Step 2: Exploring and Preparing the Data<br>Step 3: Training a Model on the Data<br>Step 4: Evaluating Model Performance<br>Practice Problem: Youth Development<br>Hierarchical Clustering<br>Gaussian Mixture Models<br>Summary<br>Assignments 13: k-Means Clustering<br>Model Performance Assessment<br>Measuring the Performance of Classification Methods<br>Binary Outcomes<br>Confusion Matrices<br>Other Measures of Performance Beyond Accuracy<br>The Kappa (κ) Statistic<br>Computation of Observed Accuracy and Expected Accuracy<br>Sensitivity and Specificity<br>Precision and Recall<br>The F-Measure<br>Visualizing Performance Tradeoffs (ROC Curve)<br>Estimating Future Performance (Internal Statistical Validation)<br>The Holdout Method<br>Cross-Validation<br>Bootstrap Sampling<br>Assignment 14: Evaluation of Model Performance<br>Improving Model Performance<br>Improving Model Performance by Parameter Tuning<br>Using caret for Automated Parameter Tuning<br>Customizing the Tuning Process<br>Improving Model Performance with Meta-learning<br>Bagging<br>Boosting<br>Random Forests<br>Adaptive Boosting<br>Assignment 15: Improving Model Performance<br>Specialized Machine Learning<br>Working with Specialized Data and Databases<br>Data Format Conversion<br>Querying Data in SQL Databases<br>Real Random Number Generation<br>Downloading the Complete Text of Web Pages<br>Reading and Writing XML with the XML Package<br>Web-Page Data Scraping<br>Parsing JSON from Web APIs<br>Reading and Writing Microsoft Excel Spreadsheets Using XLSX<br>Working with Domain-Specific Data<br>Working with Bioinformatics Data<br>Visualizing Network Data<br>Data Streaming<br>Definition<br>The stream Package<br>Synthetic Example: Random Gaussian Stream<br>Sources of Data Streams<br>Printing, Plotting and Saving Streams<br>Stream Animation<br>Case-Study: SOCR Knee Pain Data<br>Data Stream Clustering and Classification (DSC)<br>Evaluation of Data Stream Clustering<br>Optimization and Improving the Computational Performance<br>Generalizing Tabular Data Structures with dplyr<br>Making Data Frames Faster with Data.Table<br>Creating Disk-Based Data Frames with ff<br>Using Massive Matrices with bigmemory<br>Parallel Computing<br>Measuring Execution Time<br>Parallel Processing with Multiple Cores<br>Parallelization Using foreach and doParallel<br>GPU Computing<br>Deploying Optimized Learning Algorithms<br>Building Bigger Regression Models with biglm<br>Growing Bigger and Faster Random Forests with bigrf<br>Training and Evaluation Models in Parallel with caret<br>Practice Problem<br>Assignment 16: Specialized Machine Learning Topics<br>Working with Website Data<br>Network Data and Visualization<br>Variable/Feature Selection<br>Feature Selection Methods<br>Filtering Techniques<br>Wrapper Methods<br>Embedded Techniques<br>Case Study: ALS<br>Step 1: Collecting Data<br>Step 2: Exploring and Preparing the Data<br>Step 3: Training a Model on the Data<br>Step 4: Evaluating Model Performance<br>Practice Problem<br>Assignment 17: Variable/Feature Selection<br>Wrapper Feature Selection<br>Use the PPMI Dataset<br>Regularized Linear Modeling and Controlled Variable Selection<br>Questions<br>Matrix Notation<br>Regularized Linear Modeling<br>Ridge Regression<br>Least Absolute Shrinkage and Selection Operator (LASSO) Regression<br>Predictor Standardization<br>Estimation Goals<br>Linear Regression<br>Drawbacks of Linear Regression<br>Assessing Prediction Accuracy<br>Estimating the Prediction Error<br>Improving the Prediction Accuracy<br>Variable Selection<br>Regularization Framework<br>Role of the Penalty Term<br>Role of the Regularization Parameter<br>LASSO<br>General Regularization Framework<br>Implementation of Regularization<br>Example: Neuroimaging-Genetics Study of Parkinson’s Disease Dataset<br>Computational Complexity<br>LASSO and Ridge Solution Paths<br>Choice of the Regularization Parameter<br>Cross Validation Motivation<br>n-Fold Cross Validation<br>LASSO 10-Fold Cross Validation<br>Stepwise OLS (Ordinary Least Squares)<br>Final Models<br>Model Performance<br>Comparing Selected Features<br>Summary<br>Knock-off Filtering: Simulated Example<br>Notes<br>PD Neuroimaging-Genetics Case-Study<br>Fetching, Cleaning and Preparing the Data<br>Preparing the Response Vector<br>False Discovery Rate (FDR)<br>Running the Knockoff Filter<br>Assignment 18: Regularized Linear Modeling and Knockoff Filtering<br>Big Longitudinal Data Analysis<br>Time Series Analysis<br>Step 1: Plot Time Series<br>Step 2: Find Proper Parameter Values for ARIMA Model<br>Check the Differencing Parameter<br>Identifying the AR and MA Parameters<br>Step 3: Build an ARIMA Model<br>Step 4: Forecasting with ARIMA Model<br>Structural Equation Modeling (SEM)–Latent Variables<br>Foundations of SEM<br>SEM Components<br>Case Study – Parkinson’s Disease (PD)<br>Outputs of Lavaan SEM<br>Longitudinal Data Analysis–Linear Mixed Models<br>Mean Trend<br>Modeling the Correlation<br>GLMM/GEE Longitudinal Data Analysis<br>GEE Versus GLMM<br>Assignment 19: Big Longitudinal Data Analysis<br>Imaging Data<br>Time Series Analysis<br>Latent Variables Model<br>Natural Language Processing/Text Mining<br>A Simple NLP/TM Example<br>Define and Load the Unstructured-Text Documents<br>Create a New VCorpus Object<br>To-Lower Case Transformation<br>Text Pre-processing<br>Bags of Words<br>Document Term Matrix<br>Case-Study: Job Ranking<br>Step 1: Make a VCorpus Object<br>Step 2: Clean the VCorpus Object<br>Step 3: Build the Document Term Matrix<br>Area Under the ROC Curve<br>TF-IDF<br>Term Frequency (TF)<br>Inverse Document Frequency (IDF)<br>TF-IDF<br>Cosine Similarity<br>Sentiment Analysis<br>Data Preprocessing<br>NLP/TM Analytics<br>Prediction Optimization<br>Assignment 20: Natural Language Processing/Text Mining<br>Mining Twitter Data<br>Mining Cancer Clinical Notes<br>Prediction and Internal Statistical Cross Validation<br>Forecasting Types and Assessment Approaches<br>Overfitting<br>Example (US Presidential Elections)<br>Example (Google Flu Trends)<br>Example (Autism)<br>Internal Statistical Cross-Validation is an Iterative Process<br>Example (Linear Regression)<br>Cross-Validation Methods<br>Exhaustive Cross-Validation<br>Non-Exhaustive Cross-Validation<br>Case-Studies<br>Example 2: Sleep Dataset<br>Example 4: Parkinson’s Data (ppmi_data)<br>Summary of CV output<br>Alternative Predictor Functions<br>Logistic Regression<br>Quadratic Discriminant Analysis (QDA)<br>Neural Networks<br>SVM<br>k-Nearest Neighbors Algorithm (k-NN)<br>k-Means Clustering (k-MC)<br>Spectral Clustering<br>Compare the Results<br>Assignment 21: Prediction and Internal Statistical Cross-Validation<br>Function Optimization<br>Free (Unconstrained) Optimization<br>Example 1: Minimizing a Univariate Function (Inverse-CDF)<br>Example 2: Minimizing a Bivariate Function<br>Constrained Optimization<br>Equality Constraints<br>Lagrange Multipliers<br>Inequality Constrained Optimization<br>Quadratic Programming (QP)<br>General Non-linear Optimization<br>Dual Problem Optimization<br>Manual Versus Automated Lagrange Multiplier Optimization<br>Data Denoising<br>Assignment 22: Function Optimization<br>Unconstrained Optimization<br>Linear Programming (LP)<br>Mixed Integer Linear Programming (MILP)<br>Quadratic Programming (QP)<br>Complex Non-linear Optimization<br>Data Denoising<br>Deep Learning, Neural Networks<br>Deep Learning Training<br>Perceptrons<br>Biological Relevance<br>Simple Neural Net Examples<br>Exclusive OR (XOR) Operator<br>NAND Operator<br>Complex Networks Designed Using Simple Building Blocks<br>Classification<br>Sonar Data Example<br>MXNet Notes<br>Case-Studies<br>ALS Regression Example<br>Spirals 2D Data<br>IBS Study<br>Country QoL Ranking Data<br>Handwritten Digits Classification<br>Classifying Real-World Images<br>Load the Pre-trained Model<br>Load, Preprocess and Classify New Images – US Weather Pattern<br>Lake Mapourika, New Zealand<br>Beach Image<br>Volcano<br>Brain Surface<br>Face Mask<br>Assignment 23: Deep Learning, Neural Networks<br>Deep Learning Classification<br>Deep Learning Regression<br>Image Classification</div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book.show_toc(\"indented\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "qHn6o-mHl8HW",
        "outputId": "89e99bcf-d15e-4196-b47c-e3e585b995b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"max-height: 400px; overflow-y: auto; white-space: pre; font-family: monospace;\">  Motivation<br>    DSPA Mission and Objectives<br>    Examples of Driving Motivational Problems and Challenges<br>      Alzheimer’s Disease<br>      Parkinson’s Disease<br>      Drug and Substance Use<br>      Amyotrophic Lateral Sclerosis<br>      Normal Brain Visualization<br>      Neurodegeneration<br>      Genetic Forensics: 2013–2016 Ebola Outbreak<br>      Next Generation Sequence (NGS) Analysis<br>      Neuroimaging-Genetics<br>    Common Characteristics of Big (Biomedical and Health) Data<br>    Data Science<br>    Predictive Analytics<br>    High-Throughput Big Data Analytics<br>    Examples of Data Repositories, Archives, and Services<br>    DSPA Expectations<br>  Foundations of R<br>    Why Use R?<br>    Getting Started<br>      Install Basic Shell-Based R<br>      GUI Based R Invocation (RStudio)<br>      RStudio GUI Layout<br>      Some Notes<br>    Help<br>    Simple Wide-to-Long Data format Translation<br>    Data Generation<br>    Input/Output (I/O)<br>    Slicing and Extracting Data<br>    Variable Conversion<br>    Variable Information<br>    Data Selection and Manipulation<br>    Math Functions<br>    Matrix Operations<br>    Advanced Data Processing<br>    Strings<br>    Plotting<br>    QQ Normal Probability Plot<br>    Low-Level Plotting Commands<br>    Graphics Parameters<br>    Optimization and Model Fitting<br>    Statistics<br>    Distributions<br>      Programming<br>    Data Simulation Primer<br>    Appendix<br>      HTML SOCR Data Import<br>      R Debugging<br>    Assignments: 2. R Foundations<br>      Confirm that You Have Installed R/RStudio<br>      Long-to-Wide Data Format Translation<br>      Data Frames<br>      Data Stratification<br>      Simulation<br>      Programming<br>  Managing Data in R<br>    Saving and Loading R Data Structures<br>    Importing and Saving Data from CSV Files<br>    Exploring the Structure of Data<br>    Exploring Numeric Variables<br>    Measuring the Central Tendency: Mean, Median, Mode<br>    Measuring Spread: Quartiles and the Five-Number Summary<br>    Visualizing Numeric Variables: Boxplots<br>    Visualizing Numeric Variables: Histograms<br>    Understanding Numeric Data: Uniform and Normal Distributions<br>    Measuring Spread: Variance and Standard Deviation<br>    Exploring Categorical Variables<br>    Exploring Relationships Between Variables<br>    Missing Data<br>      Simulate Some Real Multivariate Data<br>      TBI Data Example<br>      Imputation via Expectation-Maximization<br>    Parsing Webpages and Visualizing Tabular HTML Data<br>    Cohort-Rebalancing (for Imbalanced Groups)<br>    Appendix<br>      Importing Data from SQL Databases<br>      R Code Fragments<br>    Assignments: 3. Managing Data in R<br>      Import, Plot, Summarize and Save Data<br>      Explore some Bivariate Relations in the Data<br>      Missing Data<br>      Surface Plots<br>      Unbalanced Designs<br>      Aggregate Analysis<br>  Data Visualization<br>    Common Questions<br>    Classification of Visualization Methods<br>    Composition<br>      Histograms and Density Plots<br>      Pie Chart<br>      Heat Map<br>    Comparison<br>      Paired Scatter Plots<br>      Jitter Plot<br>      Bar Plots<br>      Trees and Graphs<br>      Correlation Plots<br>    Relationships<br>      Line Plots Using ggplot<br>      Density Plots<br>      Distributions<br>      2D Kernel Density and 3D Surface Plots<br>      Multiple 2D Image Surface Plots<br>      3D and 4D Visualizations<br>    Appendix<br>      Hands-on Activity (Health Behavior Risks)<br>      Additional ggplot Examples<br>    Assignments 4: Data Visualization<br>      Common Plots<br>      Trees and Graphs<br>      Exploratory Data Analytics (EDA)<br>  Linear Algebra & Matrix Computing<br>    Matrices (Second Order Tensors)<br>      Create Matrices<br>      Adding Columns and Rows<br>    Matrix Subscripts<br>    Matrix Operations<br>      Addition<br>      Subtraction<br>      Multiplication<br>      Element-wise Division<br>      Transpose<br>      Multiplicative Inverse<br>    Matrix Algebra Notation<br>      Linear Models<br>      Solving Systems of Equations<br>      The Identity Matrix<br>    Scalars, Vectors and Matrices<br>      Sample Statistics (Mean, Variance)<br>      Least Square Estimation<br>    Eigenvalues and Eigenvectors<br>    Other Important Functions<br>    Matrix Notation (Another View)<br>    Multivariate Linear Regression<br>    Sample Covariance Matrix<br>    Assignments: 5. Linear Algebra & Matrix Computing<br>      How Is Matrix Multiplication Defined?<br>      Scalar Versus Matrix Multiplication<br>      Matrix Equations<br>      Least Square Estimation<br>      Matrix Manipulation<br>      Matrix Transpose<br>      Sample Statistics<br>      Least Square Estimation<br>      Eigenvalues and Eigenvectors<br>  Dimensionality Reduction<br>    Example: Reducing 2D to 1D<br>    Matrix Rotations<br>    Notation<br>    Summary (PCA vs. ICA vs. FA)<br>    Principal Component Analysis (PCA)<br>      Principal Components<br>    Independent Component Analysis (ICA)<br>    Factor Analysis (FA)<br>    Singular Value Decomposition (SVD)<br>    SVD Summary<br>    Case Study for Dimension Reduction (Parkinson’s Disease)<br>    Assignments: 6. Dimensionality Reduction<br>      Parkinson’s Disease Example<br>      Allometric Relations in Plants Example<br>  Lazy Learning: Classification Using Nearest Neighbors<br>    Motivation<br>    The kNN Algorithm Overview<br>      Distance Function and Dummy Coding<br>      Ways to Determine k<br>      Rescaling of the Features<br>      Rescaling Formulas<br>    Case Study<br>      Step 1: Collecting Data<br>      Step 2: Exploring and Preparing the Data<br>      Normalizing Data<br>      Data Preparation: Creating Training and Testing Datasets<br>      Step 3: Training a Model On the Data<br>      Step 4: Evaluating Model Performance<br>      Step 5: Improving Model Performance<br>      Testing Alternative Values of k<br>      Quantitative Assessment (Tables 7.2 and 7.3)<br>    Assignments: 7. Lazy Learning: Classification Using Nearest Neighbors<br>      Traumatic Brain Injury (TBI)<br>      Parkinson’s Disease<br>      KNN Classification in a High Dimensional Space<br>      KNN Classification in a Low Dimensional Space<br>  Probabilistic Learning: Classification Using Naive Bayes<br>    Overview of the Naive Bayes Algorithm<br>    Assumptions<br>    Bayes Formula<br>    The Laplace Estimator<br>    Case Study: Head and Neck Cancer Medication<br>      Step 1: Collecting Data<br>      Step 2: Exploring and Preparing the Data<br>      Step 3: Training a Model on the Data<br>      Step 4: Evaluating Model Performance<br>      Step 5: Improving Model Performance<br>      Step 6: Compare Naive Bayesian against LDA<br>    Practice Problem<br>    Assignments 8: Probabilistic Learning: Classification Using Naive Bayes<br>      Explain These Two Concepts<br>      Analyzing Textual Data<br>  Decision Tree Divide and Conquer Classification<br>    Motivation<br>    Hands-on Example: Iris Data<br>    Decision Tree Overview<br>      Divide and Conquer<br>      Entropy<br>      Misclassification Error and Gini Index<br>      C5.0 Decision Tree Algorithm<br>      Pruning the Decision Tree<br>    Case Study 1: Quality of Life and Chronic Disease<br>      Step 1: Collecting Data<br>      Step 2: Exploring and Preparing the Data<br>      Step 3: Training a Model On the Data<br>      Step 4: Evaluating Model Performance<br>      Step 5: Trial Option<br>      Loading the Misclassification Error Matrix<br>      Parameter Tuning<br>    Compare Different Impurity Indices<br>    Classification Rules<br>      Separate and Conquer<br>      The One Rule Algorithm<br>      The RIPPER Algorithm<br>    Case Study 2: QoL in Chronic Disease (Take 2)<br>      Step 3: Training a Model on the Data<br>      Step 4: Evaluating Model Performance<br>      Step 5: Alternative Model1<br>      Step 5: Alternative Model2<br>    Practice Problem<br>    Assignments 9: Decision Tree Divide and Conquer Classification<br>      Explain These Concepts<br>      Decision Tree Partitioning<br>  Forecasting Numeric Data Using Regression Models<br>    Understanding Regression<br>      Simple Linear Regression<br>    Ordinary Least Squares Estimation<br>      Model Assumptions<br>      Correlations<br>      Multiple Linear Regression<br>    Case Study 1: Baseball Players<br>      Step 1: Collecting Data<br>      Step 2: Exploring and Preparing the Data<br>      Exploring Relationships Among Features: The Correlation Matrix<br>      Visualizing Relationships Among Features: The Scatterplot Matrix<br>      Step 3: Training a Model on the Data<br>      Step 4: Evaluating Model Performance<br>    Step 5: Improving Model Performance<br>      Model Specification: Adding Non-linear Relationships<br>      Transformation: Converting a Numeric Variable to a Binary Indicator<br>      Model Specification: Adding Interaction Effects<br>    Understanding Regression Trees and Model Trees<br>      Adding Regression to Trees<br>    Case Study 2: Baseball Players (Take 2)<br>      Step 2: Exploring and Preparing the Data<br>      Step 3: Training a Model On the Data<br>      Visualizing Decision Trees<br>      Step 4: Evaluating Model Performance<br>      Measuring Performance with Mean Absolute Error<br>      Step 5: Improving Model Performance<br>    Practice Problem: Heart Attack Data<br>    Assignments 10: Forecasting Numeric Data Using Regression Models<br>  Black Box Machine-Learning Methods: Neural Networks and Support Vector Machines<br>    Understanding Neural Networks<br>      From Biological to Artificial Neurons<br>      Activation Functions<br>      Network Topology<br>      The Direction of Information Travel<br>      The Number of Nodes in Each Layer<br>      Training Neural Networks with Backpropagation<br>    Case Study 1: Google Trends and the Stock Market: Regression<br>      Step 1: Collecting Data<br>      Step 2: Exploring and Preparing the Data<br>      Step 3: Training a Model on the Data<br>      Step 4: Evaluating Model Performance<br>      Step 5: Improving Model Performance<br>      Step 6: Adding Additional Layers<br>    Simple NN Demo: Learning to Compute √<br>    Case Study 2: Google Trends and the Stock Market – Classification<br>    Support Vector Machines (SVM)<br>      Classification with Hyperplanes<br>    Case Study 3: Optical Character Recognition (OCR)<br>      Step 1: Prepare and Explore the Data<br>      Step 2: Training an SVM Model<br>      Step 3: Evaluating Model Performance<br>      Step 4: Improving Model Performance<br>    Case Study 4: Iris Flowers<br>      Step 1: Collecting Data<br>      Step 2: Exploring and Preparing the Data<br>      Step 3: Training a Model on the Data<br>      Step 4: Evaluating Model Performance<br>      Step 5: RBF Kernel Function<br>      Parameter Tuning<br>      Improving the Performance of Gaussian Kernels<br>    Practice<br>      Problem 1: Google Trends and the Stock Market<br>      Problem 2: Quality of Life and Chronic Disease<br>    Appendix<br>    Assignments 11: Black Box Machine-Learning Methods: Neural Networks and Support Vector Machines<br>      Learn and Predict a Power-Function<br>      Pediatric Schizophrenia Study<br>  Apriori Association Rules Learning<br>    Association Rules<br>    The Apriori Algorithm for Association Rule Learning<br>    Measuring Rule Importance by Using Support and Confidence<br>    Building a Set of Rules with the Apriori Principle<br>    A Toy Example<br>    Case Study 1: Head and Neck Cancer Medications<br>      Step 1: Collecting Data<br>      Step 2: Exploring and Preparing the Data<br>      Step 3: Training a Model on the Data<br>      Step 4: Evaluating Model Performance<br>      Step 5: Improving Model Performance<br>    Practice Problems: Groceries<br>    Summary<br>    Assignments 12: Apriori Association Rules Learning<br>  k-Means Clustering<br>    Clustering as a Machine Learning Task<br>    Silhouette Plots<br>    The k-Means Clustering Algorithm<br>      Using Distance to Assign and Update Clusters<br>      Choosing the Appropriate Number of Clusters<br>    Case Study 1: Divorce and Consequences on Young Adults<br>      Step 1: Collecting Data<br>      Step 2: Exploring and Preparing the Data<br>      Step 3: Training a Model on the Data<br>      Step 4: Evaluating Model Performance<br>      Step 5: Usage of Cluster Information<br>    Model Improvement<br>      Tuning the Parameter k<br>    Case Study 2: Pediatric Trauma<br>      Step 1: Collecting Data<br>      Step 2: Exploring and Preparing the Data<br>      Step 3: Training a Model on the Data<br>      Step 4: Evaluating Model Performance<br>      Practice Problem: Youth Development<br>    Hierarchical Clustering<br>    Gaussian Mixture Models<br>    Summary<br>    Assignments 13: k-Means Clustering<br>  Model Performance Assessment<br>    Measuring the Performance of Classification Methods<br>      Binary Outcomes<br>      Confusion Matrices<br>      Other Measures of Performance Beyond Accuracy<br>      The Kappa (κ) Statistic<br>      Computation of Observed Accuracy and Expected Accuracy<br>      Sensitivity and Specificity<br>      Precision and Recall<br>      The F-Measure<br>    Visualizing Performance Tradeoffs (ROC Curve)<br>    Estimating Future Performance (Internal Statistical Validation)<br>      The Holdout Method<br>      Cross-Validation<br>      Bootstrap Sampling<br>    Assignment 14: Evaluation of Model Performance<br>  Improving Model Performance<br>    Improving Model Performance by Parameter Tuning<br>    Using caret for Automated Parameter Tuning<br>      Customizing the Tuning Process<br>      Improving Model Performance with Meta-learning<br>      Bagging<br>      Boosting<br>      Random Forests<br>      Adaptive Boosting<br>    Assignment 15: Improving Model Performance<br>  Specialized Machine Learning<br>    Working with Specialized Data and Databases<br>      Data Format Conversion<br>      Querying Data in SQL Databases<br>      Real Random Number Generation<br>      Downloading the Complete Text of Web Pages<br>      Reading and Writing XML with the XML Package<br>      Web-Page Data Scraping<br>      Parsing JSON from Web APIs<br>      Reading and Writing Microsoft Excel Spreadsheets Using XLSX<br>    Working with Domain-Specific Data<br>      Working with Bioinformatics Data<br>      Visualizing Network Data<br>    Data Streaming<br>      Definition<br>      The stream Package<br>      Synthetic Example: Random Gaussian Stream<br>      Sources of Data Streams<br>      Printing, Plotting and Saving Streams<br>      Stream Animation<br>      Case-Study: SOCR Knee Pain Data<br>      Data Stream Clustering and Classification (DSC)<br>      Evaluation of Data Stream Clustering<br>    Optimization and Improving the Computational Performance<br>      Generalizing Tabular Data Structures with dplyr<br>      Making Data Frames Faster with Data.Table<br>      Creating Disk-Based Data Frames with ff<br>      Using Massive Matrices with bigmemory<br>    Parallel Computing<br>      Measuring Execution Time<br>      Parallel Processing with Multiple Cores<br>      Parallelization Using foreach and doParallel<br>      GPU Computing<br>    Deploying Optimized Learning Algorithms<br>      Building Bigger Regression Models with biglm<br>      Growing Bigger and Faster Random Forests with bigrf<br>      Training and Evaluation Models in Parallel with caret<br>    Practice Problem<br>    Assignment 16: Specialized Machine Learning Topics<br>      Working with Website Data<br>      Network Data and Visualization<br>  Variable/Feature Selection<br>    Feature Selection Methods<br>      Filtering Techniques<br>      Wrapper Methods<br>      Embedded Techniques<br>    Case Study: ALS<br>      Step 1: Collecting Data<br>      Step 2: Exploring and Preparing the Data<br>      Step 3: Training a Model on the Data<br>      Step 4: Evaluating Model Performance<br>    Practice Problem<br>    Assignment 17: Variable/Feature Selection<br>      Wrapper Feature Selection<br>      Use the PPMI Dataset<br>  Regularized Linear Modeling and Controlled Variable Selection<br>    Questions<br>    Matrix Notation<br>    Regularized Linear Modeling<br>      Ridge Regression<br>      Least Absolute Shrinkage and Selection Operator (LASSO) Regression<br>      Predictor Standardization<br>      Estimation Goals<br>    Linear Regression<br>      Drawbacks of Linear Regression<br>      Assessing Prediction Accuracy<br>      Estimating the Prediction Error<br>      Improving the Prediction Accuracy<br>      Variable Selection<br>    Regularization Framework<br>      Role of the Penalty Term<br>      Role of the Regularization Parameter<br>      LASSO<br>      General Regularization Framework<br>    Implementation of Regularization<br>      Example: Neuroimaging-Genetics Study of Parkinson’s Disease Dataset<br>      Computational Complexity<br>      LASSO and Ridge Solution Paths<br>      Choice of the Regularization Parameter<br>      Cross Validation Motivation<br>      n-Fold Cross Validation<br>      LASSO 10-Fold Cross Validation<br>      Stepwise OLS (Ordinary Least Squares)<br>      Final Models<br>      Model Performance<br>      Comparing Selected Features<br>      Summary<br>    Knock-off Filtering: Simulated Example<br>      Notes<br>    PD Neuroimaging-Genetics Case-Study<br>      Fetching, Cleaning and Preparing the Data<br>      Preparing the Response Vector<br>      False Discovery Rate (FDR)<br>      Running the Knockoff Filter<br>    Assignment 18: Regularized Linear Modeling and Knockoff Filtering<br>  Big Longitudinal Data Analysis<br>    Time Series Analysis<br>      Step 1: Plot Time Series<br>      Step 2: Find Proper Parameter Values for ARIMA Model<br>      Check the Differencing Parameter<br>      Identifying the AR and MA Parameters<br>      Step 3: Build an ARIMA Model<br>      Step 4: Forecasting with ARIMA Model<br>    Structural Equation Modeling (SEM)–Latent Variables<br>      Foundations of SEM<br>      SEM Components<br>      Case Study – Parkinson’s Disease (PD)<br>      Outputs of Lavaan SEM<br>    Longitudinal Data Analysis–Linear Mixed Models<br>      Mean Trend<br>      Modeling the Correlation<br>    GLMM/GEE Longitudinal Data Analysis<br>      GEE Versus GLMM<br>    Assignment 19: Big Longitudinal Data Analysis<br>      Imaging Data<br>      Time Series Analysis<br>      Latent Variables Model<br>  Natural Language Processing/Text Mining<br>    A Simple NLP/TM Example<br>      Define and Load the Unstructured-Text Documents<br>      Create a New VCorpus Object<br>      To-Lower Case Transformation<br>      Text Pre-processing<br>      Bags of Words<br>      Document Term Matrix<br>    Case-Study: Job Ranking<br>      Step 1: Make a VCorpus Object<br>      Step 2: Clean the VCorpus Object<br>      Step 3: Build the Document Term Matrix<br>      Area Under the ROC Curve<br>    TF-IDF<br>      Term Frequency (TF)<br>      Inverse Document Frequency (IDF)<br>      TF-IDF<br>    Cosine Similarity<br>    Sentiment Analysis<br>      Data Preprocessing<br>      NLP/TM Analytics<br>      Prediction Optimization<br>    Assignment 20: Natural Language Processing/Text Mining<br>      Mining Twitter Data<br>      Mining Cancer Clinical Notes<br>  Prediction and Internal Statistical Cross Validation<br>    Forecasting Types and Assessment Approaches<br>    Overfitting<br>      Example (US Presidential Elections)<br>      Example (Google Flu Trends)<br>      Example (Autism)<br>    Internal Statistical Cross-Validation is an Iterative Process<br>    Example (Linear Regression)<br>      Cross-Validation Methods<br>      Exhaustive Cross-Validation<br>      Non-Exhaustive Cross-Validation<br>    Case-Studies<br>      Example 2: Sleep Dataset<br>      Example 4: Parkinson’s Data (ppmi_data)<br>    Summary of CV output<br>    Alternative Predictor Functions<br>      Logistic Regression<br>      Quadratic Discriminant Analysis (QDA)<br>      Neural Networks<br>      SVM<br>      k-Nearest Neighbors Algorithm (k-NN)<br>      k-Means Clustering (k-MC)<br>      Spectral Clustering<br>    Compare the Results<br>    Assignment 21: Prediction and Internal Statistical Cross-Validation<br>  Function Optimization<br>    Free (Unconstrained) Optimization<br>      Example 1: Minimizing a Univariate Function (Inverse-CDF)<br>      Example 2: Minimizing a Bivariate Function<br>    Constrained Optimization<br>      Equality Constraints<br>      Lagrange Multipliers<br>      Inequality Constrained Optimization<br>      Quadratic Programming (QP)<br>    General Non-linear Optimization<br>      Dual Problem Optimization<br>    Manual Versus Automated Lagrange Multiplier Optimization<br>    Data Denoising<br>    Assignment 22: Function Optimization<br>      Unconstrained Optimization<br>      Linear Programming (LP)<br>      Mixed Integer Linear Programming (MILP)<br>      Quadratic Programming (QP)<br>      Complex Non-linear Optimization<br>      Data Denoising<br>  Deep Learning, Neural Networks<br>    Deep Learning Training<br>      Perceptrons<br>    Biological Relevance<br>    Simple Neural Net Examples<br>      Exclusive OR (XOR) Operator<br>      NAND Operator<br>      Complex Networks Designed Using Simple Building Blocks<br>    Classification<br>      Sonar Data Example<br>      MXNet Notes<br>    Case-Studies<br>      ALS Regression Example<br>      Spirals 2D Data<br>      IBS Study<br>      Country QoL Ranking Data<br>      Handwritten Digits Classification<br>    Classifying Real-World Images<br>      Load the Pre-trained Model<br>      Load, Preprocess and Classify New Images – US Weather Pattern<br>      Lake Mapourika, New Zealand<br>      Beach Image<br>      Volcano<br>      Brain Surface<br>      Face Mask<br>    Assignment 23: Deep Learning, Neural Networks<br>      Deep Learning Classification<br>      Deep Learning Regression<br>      Image Classification</div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book.show_toc(\"numbered\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "q-zXSCsIl-I6",
        "outputId": "30cadce7-cb9d-4c26-eb8e-1deb4db58c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"max-height: 400px; overflow-y: auto; white-space: pre; font-family: monospace;\">  1 Motivation<br>    1.1 DSPA Mission and Objectives<br>    1.2 Examples of Driving Motivational Problems and Challenges<br>      1.2.1 Alzheimer’s Disease<br>      1.2.2 Parkinson’s Disease<br>      1.2.3 Drug and Substance Use<br>      1.2.4 Amyotrophic Lateral Sclerosis<br>      1.2.5 Normal Brain Visualization<br>      1.2.6 Neurodegeneration<br>      1.2.7 Genetic Forensics: 2013–2016 Ebola Outbreak<br>      1.2.8 Next Generation Sequence (NGS) Analysis<br>      1.2.9 Neuroimaging-Genetics<br>    1.3 Common Characteristics of Big (Biomedical and Health) Data<br>    1.4 Data Science<br>    1.5 Predictive Analytics<br>    1.6 High-Throughput Big Data Analytics<br>    1.7 Examples of Data Repositories, Archives, and Services<br>    1.8 DSPA Expectations<br>  2 Foundations of R<br>    2.1 Why Use R?<br>    2.2 Getting Started<br>      2.2.1 Install Basic Shell-Based R<br>      2.2.2 GUI Based R Invocation (RStudio)<br>      2.2.3 RStudio GUI Layout<br>      2.2.4 Some Notes<br>    2.3 Help<br>    2.4 Simple Wide-to-Long Data format Translation<br>    2.5 Data Generation<br>    2.6 Input/Output (I/O)<br>    2.7 Slicing and Extracting Data<br>    2.8 Variable Conversion<br>    2.9 Variable Information<br>    2.10 Data Selection and Manipulation<br>    2.11 Math Functions<br>    2.12 Matrix Operations<br>    2.13 Advanced Data Processing<br>    2.14 Strings<br>    2.15 Plotting<br>    2.16 QQ Normal Probability Plot<br>    2.17 Low-Level Plotting Commands<br>    2.18 Graphics Parameters<br>    2.19 Optimization and Model Fitting<br>    2.20 Statistics<br>    2.21 Distributions<br>      2.21.1 Programming<br>    2.22 Data Simulation Primer<br>    2.23 Appendix<br>      2.23.1 HTML SOCR Data Import<br>      2.23.2 R Debugging<br>    2.24 Assignments: 2. R Foundations<br>      2.24.1 Confirm that You Have Installed R/RStudio<br>      2.24.2 Long-to-Wide Data Format Translation<br>      2.24.3 Data Frames<br>      2.24.4 Data Stratification<br>      2.24.5 Simulation<br>      2.24.6 Programming<br>  3 Managing Data in R<br>    3.1 Saving and Loading R Data Structures<br>    3.2 Importing and Saving Data from CSV Files<br>    3.3 Exploring the Structure of Data<br>    3.4 Exploring Numeric Variables<br>    3.5 Measuring the Central Tendency: Mean, Median, Mode<br>    3.6 Measuring Spread: Quartiles and the Five-Number Summary<br>    3.7 Visualizing Numeric Variables: Boxplots<br>    3.8 Visualizing Numeric Variables: Histograms<br>    3.9 Understanding Numeric Data: Uniform and Normal Distributions<br>    3.10 Measuring Spread: Variance and Standard Deviation<br>    3.11 Exploring Categorical Variables<br>    3.12 Exploring Relationships Between Variables<br>    3.13 Missing Data<br>      3.13.1 Simulate Some Real Multivariate Data<br>      3.13.2 TBI Data Example<br>      3.13.3 Imputation via Expectation-Maximization<br>    3.14 Parsing Webpages and Visualizing Tabular HTML Data<br>    3.15 Cohort-Rebalancing (for Imbalanced Groups)<br>    3.16 Appendix<br>      3.16.1 Importing Data from SQL Databases<br>      3.16.2 R Code Fragments<br>    3.17 Assignments: 3. Managing Data in R<br>      3.17.1 Import, Plot, Summarize and Save Data<br>      3.17.2 Explore some Bivariate Relations in the Data<br>      3.17.3 Missing Data<br>      3.17.4 Surface Plots<br>      3.17.5 Unbalanced Designs<br>      3.17.6 Aggregate Analysis<br>  4 Data Visualization<br>    4.1 Common Questions<br>    4.2 Classification of Visualization Methods<br>    4.3 Composition<br>      4.3.1 Histograms and Density Plots<br>      4.3.2 Pie Chart<br>      4.3.3 Heat Map<br>    4.4 Comparison<br>      4.4.1 Paired Scatter Plots<br>      4.4.2 Jitter Plot<br>      4.4.3 Bar Plots<br>      4.4.4 Trees and Graphs<br>      4.4.5 Correlation Plots<br>    4.5 Relationships<br>      4.5.1 Line Plots Using ggplot<br>      4.5.2 Density Plots<br>      4.5.3 Distributions<br>      4.5.4 2D Kernel Density and 3D Surface Plots<br>      4.5.5 Multiple 2D Image Surface Plots<br>      4.5.6 3D and 4D Visualizations<br>    4.6 Appendix<br>      4.6.1 Hands-on Activity (Health Behavior Risks)<br>      4.6.2 Additional ggplot Examples<br>    4.7 Assignments 4: Data Visualization<br>      4.7.1 Common Plots<br>      4.7.2 Trees and Graphs<br>      4.7.3 Exploratory Data Analytics (EDA)<br>  5 Linear Algebra & Matrix Computing<br>    5.1 Matrices (Second Order Tensors)<br>      5.1.1 Create Matrices<br>      5.1.2 Adding Columns and Rows<br>    5.2 Matrix Subscripts<br>    5.3 Matrix Operations<br>      5.3.1 Addition<br>      5.3.2 Subtraction<br>      5.3.3 Multiplication<br>      5.3.4 Element-wise Division<br>      5.3.5 Transpose<br>      5.3.6 Multiplicative Inverse<br>    5.4 Matrix Algebra Notation<br>      5.4.1 Linear Models<br>      5.4.2 Solving Systems of Equations<br>      5.4.3 The Identity Matrix<br>    5.5 Scalars, Vectors and Matrices<br>      5.5.1 Sample Statistics (Mean, Variance)<br>      5.5.2 Least Square Estimation<br>    5.6 Eigenvalues and Eigenvectors<br>    5.7 Other Important Functions<br>    5.8 Matrix Notation (Another View)<br>    5.9 Multivariate Linear Regression<br>    5.10 Sample Covariance Matrix<br>    5.11 Assignments: 5. Linear Algebra & Matrix Computing<br>      5.11.1 How Is Matrix Multiplication Defined?<br>      5.11.2 Scalar Versus Matrix Multiplication<br>      5.11.3 Matrix Equations<br>      5.11.4 Least Square Estimation<br>      5.11.5 Matrix Manipulation<br>      5.11.6 Matrix Transpose<br>      5.11.7 Sample Statistics<br>      5.11.8 Least Square Estimation<br>      5.11.9 Eigenvalues and Eigenvectors<br>  6 Dimensionality Reduction<br>    6.1 Example: Reducing 2D to 1D<br>    6.2 Matrix Rotations<br>    6.3 Notation<br>    6.4 Summary (PCA vs. ICA vs. FA)<br>    6.5 Principal Component Analysis (PCA)<br>      6.5.1 Principal Components<br>    6.6 Independent Component Analysis (ICA)<br>    6.7 Factor Analysis (FA)<br>    6.8 Singular Value Decomposition (SVD)<br>    6.9 SVD Summary<br>    6.10 Case Study for Dimension Reduction (Parkinson’s Disease)<br>    6.11 Assignments: 6. Dimensionality Reduction<br>      6.11.1 Parkinson’s Disease Example<br>      6.11.2 Allometric Relations in Plants Example<br>  7 Lazy Learning: Classification Using Nearest Neighbors<br>    7.1 Motivation<br>    7.2 The kNN Algorithm Overview<br>      7.2.1 Distance Function and Dummy Coding<br>      7.2.2 Ways to Determine k<br>      7.2.3 Rescaling of the Features<br>      7.2.4 Rescaling Formulas<br>    7.3 Case Study<br>      7.3.1 Step 1: Collecting Data<br>      7.3.2 Step 2: Exploring and Preparing the Data<br>      7.3.3 Normalizing Data<br>      7.3.4 Data Preparation: Creating Training and Testing Datasets<br>      7.3.5 Step 3: Training a Model On the Data<br>      7.3.6 Step 4: Evaluating Model Performance<br>      7.3.7 Step 5: Improving Model Performance<br>      7.3.8 Testing Alternative Values of k<br>      7.3.9 Quantitative Assessment (Tables 7.2 and 7.3)<br>    7.4 Assignments: 7. Lazy Learning: Classification Using Nearest Neighbors<br>      7.4.1 Traumatic Brain Injury (TBI)<br>      7.4.2 Parkinson’s Disease<br>      7.4.3 KNN Classification in a High Dimensional Space<br>      7.4.4 KNN Classification in a Low Dimensional Space<br>  8 Probabilistic Learning: Classification Using Naive Bayes<br>    8.1 Overview of the Naive Bayes Algorithm<br>    8.2 Assumptions<br>    8.3 Bayes Formula<br>    8.4 The Laplace Estimator<br>    8.5 Case Study: Head and Neck Cancer Medication<br>      8.5.1 Step 1: Collecting Data<br>      8.5.2 Step 2: Exploring and Preparing the Data<br>      8.5.3 Step 3: Training a Model on the Data<br>      8.5.4 Step 4: Evaluating Model Performance<br>      8.5.5 Step 5: Improving Model Performance<br>      8.5.6 Step 6: Compare Naive Bayesian against LDA<br>    8.6 Practice Problem<br>    8.7 Assignments 8: Probabilistic Learning: Classification Using Naive Bayes<br>      8.7.1 Explain These Two Concepts<br>      8.7.2 Analyzing Textual Data<br>  9 Decision Tree Divide and Conquer Classification<br>    9.1 Motivation<br>    9.2 Hands-on Example: Iris Data<br>    9.3 Decision Tree Overview<br>      9.3.1 Divide and Conquer<br>      9.3.2 Entropy<br>      9.3.3 Misclassification Error and Gini Index<br>      9.3.4 C5.0 Decision Tree Algorithm<br>      9.3.5 Pruning the Decision Tree<br>    9.4 Case Study 1: Quality of Life and Chronic Disease<br>      9.4.1 Step 1: Collecting Data<br>      9.4.2 Step 2: Exploring and Preparing the Data<br>      9.4.3 Step 3: Training a Model On the Data<br>      9.4.4 Step 4: Evaluating Model Performance<br>      9.4.5 Step 5: Trial Option<br>      9.4.6 Loading the Misclassification Error Matrix<br>      9.4.7 Parameter Tuning<br>    9.5 Compare Different Impurity Indices<br>    9.6 Classification Rules<br>      9.6.1 Separate and Conquer<br>      9.6.2 The One Rule Algorithm<br>      9.6.3 The RIPPER Algorithm<br>    9.7 Case Study 2: QoL in Chronic Disease (Take 2)<br>      9.7.1 Step 3: Training a Model on the Data<br>      9.7.2 Step 4: Evaluating Model Performance<br>      9.7.3 Step 5: Alternative Model1<br>      9.7.4 Step 5: Alternative Model2<br>    9.8 Practice Problem<br>    9.9 Assignments 9: Decision Tree Divide and Conquer Classification<br>      9.9.1 Explain These Concepts<br>      9.9.2 Decision Tree Partitioning<br>  10 Forecasting Numeric Data Using Regression Models<br>    10.1 Understanding Regression<br>      10.1.1 Simple Linear Regression<br>    10.2 Ordinary Least Squares Estimation<br>      10.2.1 Model Assumptions<br>      10.2.2 Correlations<br>      10.2.3 Multiple Linear Regression<br>    10.3 Case Study 1: Baseball Players<br>      10.3.1 Step 1: Collecting Data<br>      10.3.2 Step 2: Exploring and Preparing the Data<br>      10.3.3 Exploring Relationships Among Features: The Correlation Matrix<br>      10.3.4 Visualizing Relationships Among Features: The Scatterplot Matrix<br>      10.3.5 Step 3: Training a Model on the Data<br>      10.3.6 Step 4: Evaluating Model Performance<br>    10.4 Step 5: Improving Model Performance<br>      10.4.1 Model Specification: Adding Non-linear Relationships<br>      10.4.2 Transformation: Converting a Numeric Variable to a Binary Indicator<br>      10.4.3 Model Specification: Adding Interaction Effects<br>    10.5 Understanding Regression Trees and Model Trees<br>      10.5.1 Adding Regression to Trees<br>    10.6 Case Study 2: Baseball Players (Take 2)<br>      10.6.1 Step 2: Exploring and Preparing the Data<br>      10.6.2 Step 3: Training a Model On the Data<br>      10.6.3 Visualizing Decision Trees<br>      10.6.4 Step 4: Evaluating Model Performance<br>      10.6.5 Measuring Performance with Mean Absolute Error<br>      10.6.6 Step 5: Improving Model Performance<br>    10.7 Practice Problem: Heart Attack Data<br>    10.8 Assignments 10: Forecasting Numeric Data Using Regression Models<br>  11 Black Box Machine-Learning Methods: Neural Networks and Support Vector Machines<br>    11.1 Understanding Neural Networks<br>      11.1.1 From Biological to Artificial Neurons<br>      11.1.2 Activation Functions<br>      11.1.3 Network Topology<br>      11.1.4 The Direction of Information Travel<br>      11.1.5 The Number of Nodes in Each Layer<br>      11.1.6 Training Neural Networks with Backpropagation<br>    11.2 Case Study 1: Google Trends and the Stock Market: Regression<br>      11.2.1 Step 1: Collecting Data<br>      11.2.2 Step 2: Exploring and Preparing the Data<br>      11.2.3 Step 3: Training a Model on the Data<br>      11.2.4 Step 4: Evaluating Model Performance<br>      11.2.5 Step 5: Improving Model Performance<br>      11.2.6 Step 6: Adding Additional Layers<br>    11.3 Simple NN Demo: Learning to Compute √<br>    11.4 Case Study 2: Google Trends and the Stock Market – Classification<br>    11.5 Support Vector Machines (SVM)<br>      11.5.1 Classification with Hyperplanes<br>    11.6 Case Study 3: Optical Character Recognition (OCR)<br>      11.6.1 Step 1: Prepare and Explore the Data<br>      11.6.2 Step 2: Training an SVM Model<br>      11.6.3 Step 3: Evaluating Model Performance<br>      11.6.4 Step 4: Improving Model Performance<br>    11.7 Case Study 4: Iris Flowers<br>      11.7.1 Step 1: Collecting Data<br>      11.7.2 Step 2: Exploring and Preparing the Data<br>      11.7.3 Step 3: Training a Model on the Data<br>      11.7.4 Step 4: Evaluating Model Performance<br>      11.7.5 Step 5: RBF Kernel Function<br>      11.7.6 Parameter Tuning<br>      11.7.7 Improving the Performance of Gaussian Kernels<br>    11.8 Practice<br>      11.8.1 Problem 1: Google Trends and the Stock Market<br>      11.8.2 Problem 2: Quality of Life and Chronic Disease<br>    11.9 Appendix<br>    11.10 Assignments 11: Black Box Machine-Learning Methods: Neural Networks and Support Vector Machines<br>      11.10.1 Learn and Predict a Power-Function<br>      11.10.2 Pediatric Schizophrenia Study<br>  12 Apriori Association Rules Learning<br>    12.1 Association Rules<br>    12.2 The Apriori Algorithm for Association Rule Learning<br>    12.3 Measuring Rule Importance by Using Support and Confidence<br>    12.4 Building a Set of Rules with the Apriori Principle<br>    12.5 A Toy Example<br>    12.6 Case Study 1: Head and Neck Cancer Medications<br>      12.6.1 Step 1: Collecting Data<br>      12.6.2 Step 2: Exploring and Preparing the Data<br>      12.6.3 Step 3: Training a Model on the Data<br>      12.6.4 Step 4: Evaluating Model Performance<br>      12.6.5 Step 5: Improving Model Performance<br>    12.7 Practice Problems: Groceries<br>    12.8 Summary<br>    12.9 Assignments 12: Apriori Association Rules Learning<br>  13 k-Means Clustering<br>    13.1 Clustering as a Machine Learning Task<br>    13.2 Silhouette Plots<br>    13.3 The k-Means Clustering Algorithm<br>      13.3.1 Using Distance to Assign and Update Clusters<br>      13.3.2 Choosing the Appropriate Number of Clusters<br>    13.4 Case Study 1: Divorce and Consequences on Young Adults<br>      13.4.1 Step 1: Collecting Data<br>      13.4.2 Step 2: Exploring and Preparing the Data<br>      13.4.3 Step 3: Training a Model on the Data<br>      13.4.4 Step 4: Evaluating Model Performance<br>      13.4.5 Step 5: Usage of Cluster Information<br>    13.5 Model Improvement<br>      13.5.1 Tuning the Parameter k<br>    13.6 Case Study 2: Pediatric Trauma<br>      13.6.1 Step 1: Collecting Data<br>      13.6.2 Step 2: Exploring and Preparing the Data<br>      13.6.3 Step 3: Training a Model on the Data<br>      13.6.4 Step 4: Evaluating Model Performance<br>      13.6.5 Practice Problem: Youth Development<br>    13.7 Hierarchical Clustering<br>    13.8 Gaussian Mixture Models<br>    13.9 Summary<br>    13.10 Assignments 13: k-Means Clustering<br>  14 Model Performance Assessment<br>    14.1 Measuring the Performance of Classification Methods<br>      14.1.1 Binary Outcomes<br>      14.1.2 Confusion Matrices<br>      14.1.3 Other Measures of Performance Beyond Accuracy<br>      14.1.4 The Kappa (κ) Statistic<br>      14.1.5 Computation of Observed Accuracy and Expected Accuracy<br>      14.1.6 Sensitivity and Specificity<br>      14.1.7 Precision and Recall<br>      14.1.8 The F-Measure<br>    14.2 Visualizing Performance Tradeoffs (ROC Curve)<br>    14.3 Estimating Future Performance (Internal Statistical Validation)<br>      14.3.1 The Holdout Method<br>      14.3.2 Cross-Validation<br>      14.3.3 Bootstrap Sampling<br>    14.4 Assignment 14: Evaluation of Model Performance<br>  15 Improving Model Performance<br>    15.1 Improving Model Performance by Parameter Tuning<br>    15.2 Using caret for Automated Parameter Tuning<br>      15.2.1 Customizing the Tuning Process<br>      15.2.2 Improving Model Performance with Meta-learning<br>      15.2.3 Bagging<br>      15.2.4 Boosting<br>      15.2.5 Random Forests<br>      15.2.6 Adaptive Boosting<br>    15.3 Assignment 15: Improving Model Performance<br>  16 Specialized Machine Learning<br>    16.1 Working with Specialized Data and Databases<br>      16.1.1 Data Format Conversion<br>      16.1.2 Querying Data in SQL Databases<br>      16.1.3 Real Random Number Generation<br>      16.1.4 Downloading the Complete Text of Web Pages<br>      16.1.5 Reading and Writing XML with the XML Package<br>      16.1.6 Web-Page Data Scraping<br>      16.1.7 Parsing JSON from Web APIs<br>      16.1.8 Reading and Writing Microsoft Excel Spreadsheets Using XLSX<br>    16.2 Working with Domain-Specific Data<br>      16.2.1 Working with Bioinformatics Data<br>      16.2.2 Visualizing Network Data<br>    16.3 Data Streaming<br>      16.3.1 Definition<br>      16.3.2 The stream Package<br>      16.3.3 Synthetic Example: Random Gaussian Stream<br>      16.3.4 Sources of Data Streams<br>      16.3.5 Printing, Plotting and Saving Streams<br>      16.3.6 Stream Animation<br>      16.3.7 Case-Study: SOCR Knee Pain Data<br>      16.3.8 Data Stream Clustering and Classification (DSC)<br>      16.3.9 Evaluation of Data Stream Clustering<br>    16.4 Optimization and Improving the Computational Performance<br>      16.4.1 Generalizing Tabular Data Structures with dplyr<br>      16.4.2 Making Data Frames Faster with Data.Table<br>      16.4.3 Creating Disk-Based Data Frames with ff<br>      16.4.4 Using Massive Matrices with bigmemory<br>    16.5 Parallel Computing<br>      16.5.1 Measuring Execution Time<br>      16.5.2 Parallel Processing with Multiple Cores<br>      16.5.3 Parallelization Using foreach and doParallel<br>      16.5.4 GPU Computing<br>    16.6 Deploying Optimized Learning Algorithms<br>      16.6.1 Building Bigger Regression Models with biglm<br>      16.6.2 Growing Bigger and Faster Random Forests with bigrf<br>      16.6.3 Training and Evaluation Models in Parallel with caret<br>    16.7 Practice Problem<br>    16.8 Assignment 16: Specialized Machine Learning Topics<br>      16.8.1 Working with Website Data<br>      16.8.2 Network Data and Visualization<br>  17 Variable/Feature Selection<br>    17.1 Feature Selection Methods<br>      17.1.1 Filtering Techniques<br>      17.1.2 Wrapper Methods<br>      17.1.3 Embedded Techniques<br>    17.2 Case Study: ALS<br>      17.2.1 Step 1: Collecting Data<br>      17.2.2 Step 2: Exploring and Preparing the Data<br>      17.2.3 Step 3: Training a Model on the Data<br>      17.2.4 Step 4: Evaluating Model Performance<br>    17.3 Practice Problem<br>    17.4 Assignment 17: Variable/Feature Selection<br>      17.4.1 Wrapper Feature Selection<br>      17.4.2 Use the PPMI Dataset<br>  18 Regularized Linear Modeling and Controlled Variable Selection<br>    18.1 Questions<br>    18.2 Matrix Notation<br>    18.3 Regularized Linear Modeling<br>      18.3.1 Ridge Regression<br>      18.3.2 Least Absolute Shrinkage and Selection Operator (LASSO) Regression<br>      18.3.3 Predictor Standardization<br>      18.3.4 Estimation Goals<br>    18.4 Linear Regression<br>      18.4.1 Drawbacks of Linear Regression<br>      18.4.2 Assessing Prediction Accuracy<br>      18.4.3 Estimating the Prediction Error<br>      18.4.4 Improving the Prediction Accuracy<br>      18.4.5 Variable Selection<br>    18.5 Regularization Framework<br>      18.5.1 Role of the Penalty Term<br>      18.5.2 Role of the Regularization Parameter<br>      18.5.3 LASSO<br>      18.5.4 General Regularization Framework<br>    18.6 Implementation of Regularization<br>      18.6.1 Example: Neuroimaging-Genetics Study of Parkinson’s Disease Dataset<br>      18.6.2 Computational Complexity<br>      18.6.3 LASSO and Ridge Solution Paths<br>      18.6.4 Choice of the Regularization Parameter<br>      18.6.5 Cross Validation Motivation<br>      18.6.6 n-Fold Cross Validation<br>      18.6.7 LASSO 10-Fold Cross Validation<br>      18.6.8 Stepwise OLS (Ordinary Least Squares)<br>      18.6.9 Final Models<br>      18.6.10 Model Performance<br>      18.6.11 Comparing Selected Features<br>      18.6.12 Summary<br>    18.7 Knock-off Filtering: Simulated Example<br>      18.7.1 Notes<br>    18.8 PD Neuroimaging-Genetics Case-Study<br>      18.8.1 Fetching, Cleaning and Preparing the Data<br>      18.8.2 Preparing the Response Vector<br>      18.8.3 False Discovery Rate (FDR)<br>      18.8.4 Running the Knockoff Filter<br>    18.9 Assignment 18: Regularized Linear Modeling and Knockoff Filtering<br>  19 Big Longitudinal Data Analysis<br>    19.1 Time Series Analysis<br>      19.1.1 Step 1: Plot Time Series<br>      19.1.2 Step 2: Find Proper Parameter Values for ARIMA Model<br>      19.1.3 Check the Differencing Parameter<br>      19.1.4 Identifying the AR and MA Parameters<br>      19.1.5 Step 3: Build an ARIMA Model<br>      19.1.6 Step 4: Forecasting with ARIMA Model<br>    19.2 Structural Equation Modeling (SEM)–Latent Variables<br>      19.2.1 Foundations of SEM<br>      19.2.2 SEM Components<br>      19.2.3 Case Study – Parkinson’s Disease (PD)<br>      19.2.4 Outputs of Lavaan SEM<br>    19.3 Longitudinal Data Analysis–Linear Mixed Models<br>      19.3.1 Mean Trend<br>      19.3.2 Modeling the Correlation<br>    19.4 GLMM/GEE Longitudinal Data Analysis<br>      19.4.1 GEE Versus GLMM<br>    19.5 Assignment 19: Big Longitudinal Data Analysis<br>      19.5.1 Imaging Data<br>      19.5.2 Time Series Analysis<br>      19.5.3 Latent Variables Model<br>  20 Natural Language Processing/Text Mining<br>    20.1 A Simple NLP/TM Example<br>      20.1.1 Define and Load the Unstructured-Text Documents<br>      20.1.2 Create a New VCorpus Object<br>      20.1.3 To-Lower Case Transformation<br>      20.1.4 Text Pre-processing<br>      20.1.5 Bags of Words<br>      20.1.6 Document Term Matrix<br>    20.2 Case-Study: Job Ranking<br>      20.2.1 Step 1: Make a VCorpus Object<br>      20.2.2 Step 2: Clean the VCorpus Object<br>      20.2.3 Step 3: Build the Document Term Matrix<br>      20.2.4 Area Under the ROC Curve<br>    20.3 TF-IDF<br>      20.3.1 Term Frequency (TF)<br>      20.3.2 Inverse Document Frequency (IDF)<br>      20.3.3 TF-IDF<br>    20.4 Cosine Similarity<br>    20.5 Sentiment Analysis<br>      20.5.1 Data Preprocessing<br>      20.5.2 NLP/TM Analytics<br>      20.5.3 Prediction Optimization<br>    20.6 Assignment 20: Natural Language Processing/Text Mining<br>      20.6.1 Mining Twitter Data<br>      20.6.2 Mining Cancer Clinical Notes<br>  21 Prediction and Internal Statistical Cross Validation<br>    21.1 Forecasting Types and Assessment Approaches<br>    21.2 Overfitting<br>      21.2.1 Example (US Presidential Elections)<br>      21.2.2 Example (Google Flu Trends)<br>      21.2.3 Example (Autism)<br>    21.3 Internal Statistical Cross-Validation is an Iterative Process<br>    21.4 Example (Linear Regression)<br>      21.4.1 Cross-Validation Methods<br>      21.4.2 Exhaustive Cross-Validation<br>      21.4.3 Non-Exhaustive Cross-Validation<br>    21.5 Case-Studies<br>      21.5.1 Example 2: Sleep Dataset<br>      21.5.2 Example 4: Parkinson’s Data (ppmi_data)<br>    21.6 Summary of CV output<br>    21.7 Alternative Predictor Functions<br>      21.7.1 Logistic Regression<br>      21.7.2 Quadratic Discriminant Analysis (QDA)<br>      21.7.3 Neural Networks<br>      21.7.4 SVM<br>      21.7.5 k-Nearest Neighbors Algorithm (k-NN)<br>      21.7.6 k-Means Clustering (k-MC)<br>      21.7.7 Spectral Clustering<br>    21.8 Compare the Results<br>    21.9 Assignment 21: Prediction and Internal Statistical Cross-Validation<br>  22 Function Optimization<br>    22.1 Free (Unconstrained) Optimization<br>      22.1.1 Example 1: Minimizing a Univariate Function (Inverse-CDF)<br>      22.1.2 Example 2: Minimizing a Bivariate Function<br>    22.2 Constrained Optimization<br>      22.2.1 Equality Constraints<br>      22.2.2 Lagrange Multipliers<br>      22.2.3 Inequality Constrained Optimization<br>      22.2.4 Quadratic Programming (QP)<br>    22.3 General Non-linear Optimization<br>      22.3.1 Dual Problem Optimization<br>    22.4 Manual Versus Automated Lagrange Multiplier Optimization<br>    22.5 Data Denoising<br>    22.6 Assignment 22: Function Optimization<br>      22.6.1 Unconstrained Optimization<br>      22.6.2 Linear Programming (LP)<br>      22.6.3 Mixed Integer Linear Programming (MILP)<br>      22.6.4 Quadratic Programming (QP)<br>      22.6.5 Complex Non-linear Optimization<br>      22.6.6 Data Denoising<br>  23 Deep Learning, Neural Networks<br>    23.1 Deep Learning Training<br>      23.1.1 Perceptrons<br>    23.2 Biological Relevance<br>    23.3 Simple Neural Net Examples<br>      23.3.1 Exclusive OR (XOR) Operator<br>      23.3.2 NAND Operator<br>      23.3.3 Complex Networks Designed Using Simple Building Blocks<br>    23.4 Classification<br>      23.4.1 Sonar Data Example<br>      23.4.2 MXNet Notes<br>    23.5 Case-Studies<br>      23.5.1 ALS Regression Example<br>      23.5.2 Spirals 2D Data<br>      23.5.3 IBS Study<br>      23.5.4 Country QoL Ranking Data<br>      23.5.5 Handwritten Digits Classification<br>    23.6 Classifying Real-World Images<br>      23.6.1 Load the Pre-trained Model<br>      23.6.2 Load, Preprocess and Classify New Images – US Weather Pattern<br>      23.6.3 Lake Mapourika, New Zealand<br>      23.6.4 Beach Image<br>      23.6.5 Volcano<br>      23.6.6 Brain Surface<br>      23.6.7 Face Mask<br>    23.7 Assignment 23: Deep Learning, Neural Networks<br>      23.7.1 Deep Learning Classification<br>      23.7.2 Deep Learning Regression<br>      23.7.3 Image Classification</div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Discussion and Explanations**\n",
        "\n",
        "**Tree Design**\n",
        "\n",
        "We implemented a single node class with attributes describing the section (ex. \"1.3.7\"), title (ex. \"Data Visualization\"), page, total book sections, children, section_order, and book.  The section_order is deduced from the count of periods in each section string + 1, and describes the relative depth.\n",
        "\n",
        "The book is identified as the root node, and its attribute \"book\" is inherited by each of the sections.\n",
        "\n",
        "**Insertion Strategy**\n",
        "\n",
        "To insert, the function first checks if it's section order is one more than that of the root (book) node.  If so, it immediately appends to the root's children attrribute and increments the tree's total sections.  If the inserted node is not a direct child, the function will loop through the children of the root node and check for a match in section lineage.  Once a match is identified, the insert function is recurively called and the total is updated.  An additional method, insert_many, calls insert_section for each item in an array of contents.\n",
        "\n",
        "In this case, we are assuming that the table of contents are naturally sorted before insertion.  If a node were to be inserted before its parent node, the inserted section would not find a lineage match.  In such cases, the user would be prompted with a message indicating that the section could not be added to the contents.\n",
        "\n",
        "In the best case, an individual insertion would have a time complexity of O(1) in the instance that the inserted node is a direct child of the root.  In the worst case (assuming a very unbalanced tree), the function will recurse through every node before finding its proper parent node, resulting in a time complexity of O(n).  However, in the average case, a relatively balanced tree will only require O(log n) comparisons before placing the inserted node.\n",
        "\n",
        "**Traversal Method and Justification**\n",
        "\n",
        "We have used the preoder traversal to print the Table of Contents in the use case. The requirement is to print the contents in order and since there is hierarchy and we want to make sure that chapter -> sub-chapter -> sub-sub-chapter order is maintained in the output, we want to make sure the root/ the parent node is printed first, before we traverse its children nodes.\n",
        "Breadth-first search (BFS) would ideally first print all the chapters, the recursively consider one chapter at a time and print all its sub-chapters at all and all the sub-sub-chapters in the next iteration. This is not ideally considered to be the right approach to print the table of contents, since it will list all the available chapters first, all the sub-chapters and then go to all the sub-sub-chapters. Since we chose to consider pre-order traversal which is one of the techniques under Depth First Search (DFS) we are using a combination of pre-order + DFS methods here as our traversal methods for the Table of Contents (TOC)."
      ],
      "metadata": {
        "id": "Qq0PFYWcl6ug"
      }
    }
  ]
}